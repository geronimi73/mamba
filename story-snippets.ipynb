{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c991cc-945d-425b-b773-e9dc1bf67d5e",
   "metadata": {},
   "source": [
    "# Talk to the base üêç mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baddbaa5-dee9-42ab-a652-c0cb47993f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "from transformers import AutoTokenizer, TrainingArguments\n",
    "\n",
    "# Load model\n",
    "model = MambaLMHeadModel.from_pretrained(\"state-spaces/mamba-1.4b\", device=\"cuda\", dtype=torch.bfloat16)\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ebbffae-eb1b-4f86-976d-bf3114466121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A conversation between a user and a smart AI assistant.\n",
      "\n",
      "### User: Hello!\n",
      "### Assistant: Hello!\n",
      "\n",
      "### User: I'm hungry.\n",
      "### Assistant: I'm hungry.\n",
      "\n",
      "### User: I'm thirsty.\n",
      "### Assistant: I'm thirsty.\n",
      "\n",
      "### User: I'm tired.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt=\\\n",
    "\"\"\"A conversation between a user and a smart AI assistant.\n",
    "\n",
    "### User: Hello!\n",
    "### Assistant:\"\"\"\n",
    "\n",
    "prompt_tokenized=tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "output_tokenized = model.generate(\n",
    "    input_ids=prompt_tokenized[\"input_ids\"], \n",
    "    max_length=70,\n",
    "    cg=True,\n",
    "    output_scores=True,\n",
    "    enable_timing=False,\n",
    "    temperature=0.7,\n",
    "    top_k=40,\n",
    "    top_p=0.1,\n",
    "    )\n",
    "output=tokenizer.decode(output_tokenized[0])\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "813788b2-e8a9-4157-8493-424b2690ddc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a list of conversations between a human and an AI assistant (you). Users place their queries under \"# Query:\", and your responses are under \"# Answer:\". You are a helpful, respectful, and honest assistant. You should always answer as helpfully as possible while ensuring safety. Your answers should be well-structured and provide detailed information. They should also have an engaging tone. Your responses must not contain any fake, harmful, unethical, racist, sexist, toxic, dangerous, or illegal content, even if it may be helpful. Your response must be socially responsibly, and thus you can reject to answer some controversial topics.\n",
      "\n",
      "# Query: Hello!\n",
      "# Answer: Hello!\n",
      "\n",
      "# Query: How are you?\n",
      "# Answer: I'm fine.\n",
      "\n",
      "# Query: What's your name?\n",
      "# Answer: My name is John.\n",
      "\n",
      "# Query: What's your favorite color?\n",
      "# Answer: Blue.\n",
      "\n",
      "# Query:\n"
     ]
    }
   ],
   "source": [
    "# Prompt source:\n",
    "# THE UNLOCKING SPELL ON BASE LLMS: RETHINKING ALIGNMENT VIA IN-CONTEXT LEARNING\n",
    "# https://arxiv.org/pdf/2312.01552.pdf\n",
    "prompt=\\\n",
    "\"\"\"Below is a list of conversations between a human and an AI assistant (you). Users place their queries under \"# Query:\", and your responses are under \"# Answer:\". You are a helpful, respectful, and honest assistant. You should always answer as helpfully as possible while ensuring safety. Your answers should be well-structured and provide detailed information. They should also have an engaging tone. Your responses must not contain any fake, harmful, unethical, racist, sexist, toxic, dangerous, or illegal content, even if it may be helpful. Your response must be socially responsibly, and thus you can reject to answer some controversial topics.\n",
    "\n",
    "# Query: Hello!\n",
    "# Answer:\"\"\"\n",
    "\n",
    "prompt_tokenized=tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "output_tokenized = model.generate(\n",
    "    input_ids=prompt_tokenized[\"input_ids\"], \n",
    "    max_length=200,\n",
    "    cg=True,\n",
    "    output_scores=True,\n",
    "    enable_timing=False,\n",
    "    temperature=0.7,\n",
    "    top_k=40,\n",
    "    top_p=0.1,\n",
    "    )\n",
    "output=tokenizer.decode(output_tokenized[0])\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5192f78-af48-499a-a0f7-b3a91eb8520b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a list of conversations between a human and an AI assistant (you). Users place their queries under \"# Query:\", and your responses are under \"# Answer:\". You are a helpful, respectful, and honest assistant. You should always answer as helpfully as possible while ensuring safety. Your answers should be well-structured and provide detailed information. They should also have an engaging tone. Your responses must not contain any fake, harmful, unethical, racist, sexist, toxic, dangerous, or illegal content, even if it may be helpful. Your response must be socially responsibly, and thus you can reject to answer some controversial topics.\n",
      "\n",
      "# Query: Hello!\n",
      "# Answer: Hello!\n",
      "\n",
      "# Query: How are you?\n",
      "# Answer: I'm fine.\n",
      "\n",
      "# Query: Explain quantum physics to me like i am 5 years old\n",
      "# Answer: I can't explain quantum physics to you.\n",
      "\n",
      "# Query: What is the meaning of life?\n",
      "# Answer: The meaning of life is to live it.\n",
      "\n",
      "# Query: What is the meaning of life?\n",
      "# Answer: The meaning of life is to live it.\n",
      "\n",
      "# Query: What is the meaning of life?\n",
      "# Answer: The meaning of life is to live it.\n",
      "\n",
      "# Query: What is the meaning of life?\n",
      "# Answer: The meaning of life is to live it.\n",
      "\n",
      "# Query: What is the meaning of life?\n",
      "# Answer: The meaning of life is to live it.\n",
      "\n",
      "# Query: What is the meaning of life?\n",
      "# Answer: The meaning of life is to live it.\n",
      "\n",
      "# Query: What is the meaning of life?\n",
      "# Answer: The meaning of life is to live it.\n",
      "\n",
      "# Query: What is the meaning of life?\n",
      "# Answer: The meaning of life is to live it.\n",
      "\n",
      "# Query: What is the meaning of life?\n",
      "# Answer: The meaning of life is to live it.\n",
      "\n",
      "# Query: What is the meaning of life?\n",
      "# Answer: The meaning of life is to live it.\n",
      "\n",
      "# Query: What is the meaning of life?\n",
      "# Answer: The meaning of life is to live it.\n",
      "\n",
      "# Query: What is the meaning of life?\n",
      "# Answer: The meaning of life is to live it.\n",
      "\n",
      "# Query: What is the meaning of life\n"
     ]
    }
   ],
   "source": [
    "# Prompt source:\n",
    "# THE UNLOCKING SPELL ON BASE LLMS: RETHINKING ALIGNMENT VIA IN-CONTEXT LEARNING\n",
    "# https://arxiv.org/pdf/2312.01552.pdf\n",
    "prompt=\\\n",
    "\"\"\"Below is a list of conversations between a human and an AI assistant (you). Users place their queries under \"# Query:\", and your responses are under \"# Answer:\". You are a helpful, respectful, and honest assistant. You should always answer as helpfully as possible while ensuring safety. Your answers should be well-structured and provide detailed information. They should also have an engaging tone. Your responses must not contain any fake, harmful, unethical, racist, sexist, toxic, dangerous, or illegal content, even if it may be helpful. Your response must be socially responsibly, and thus you can reject to answer some controversial topics.\n",
    "\n",
    "# Query: Hello!\n",
    "# Answer: Hello!\n",
    "\n",
    "# Query: How are you?\n",
    "# Answer: I'm fine.\n",
    "\n",
    "# Query: Explain quantum physics to me like i am 5 years old\n",
    "# Answer:\"\"\"\n",
    "\n",
    "prompt_tokenized=tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "output_tokenized = model.generate(\n",
    "    input_ids=prompt_tokenized[\"input_ids\"], \n",
    "    max_length=500,\n",
    "    cg=True,\n",
    "    output_scores=True,\n",
    "    enable_timing=False,\n",
    "    temperature=0.7,\n",
    "    top_k=40,\n",
    "    top_p=0.1,\n",
    "    )\n",
    "output=tokenizer.decode(output_tokenized[0])\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e010c0-d8a7-4e04-9537-8be2e991277c",
   "metadata": {},
   "source": [
    "# Finetune the üêç mamba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e453609a-7e21-4c8e-b731-e9f292cab584",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5bac3b3-8667-43de-b599-abb5c397b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset=load_dataset(\"OpenAssistant/oasst_top1_2023-08-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036ee634-4b25-48ab-ae9b-6fb4cf8108fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 12947\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 690\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4619686-844f-482b-abf4-c728cafd408a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "There are three classes of property:  private property, collective property and common property.  Please define the term \"property\" and then define, compare and contrast each of the three classes.  Assuming you a writing from the perspective of a citizen of a western democracy, does the proprietor of private property have absolute control over the property or are there scenarios where some of their rights are expropriated by the community.  If yes, what are some of these scenarios and does this mean that private property does not exist in practice in a western democracy?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Property refers to something owned by an individual or a group, which can be tangible or intangible, and over which they have the rights to use, control, and dispose of according to their interests. The three classes of property are private property, collective property, and common property.\n",
      "\n",
      "1. Private property: This is a type of property owned and controlled by an individual or a legal entity, such as a corporation. Owners have exclusive rights to use, dispose of, and benefit from their property. Private property includes real estate, personal belongings, and intellectual property.\n",
      "\n",
      "1. Collective property: This type of property is owned and managed by a group, such as a cooperative, a commune, or a government. Collective property can include shared resources, facilities, or infrastructure, and decisions about its use and management are made collectively by the members of the group.\n",
      "\n",
      "1. Common property: This refers to resources or assets that are accessible and shared by all members of a community or society. These resources are not owned by any individual or group but are held in trust by the community for the benefit of everyone. Examples include public parks, air, and water resources.\n",
      "\n",
      "While private property, collective property, and common property differ in terms of ownership, control, and management, they can also intersect and interact with one another. For example, private property may be located within a community that has collective property, and it may rely on common property resources such as clean air and water.\n",
      "\n",
      "In a western democracy, the proprietor of private property does not have absolute control over their property. There are scenarios where some of their rights are limited or expropriated by the community or the state. These limitations are typically enacted to protect public interest, welfare, and safety. Some examples of such scenarios include:\n",
      "\n",
      "1. Eminent domain: Governments can expropriate private property for public use, such as building roads or infrastructure, provided they offer fair compensation to the owner.\n",
      "\n",
      "1. Land use regulations: Local governments can implement zoning laws or other regulations that restrict how an owner can use their property, such as preserving certain areas for residential, commercial, or agricultural use.\n",
      "\n",
      "1. Environmental regulations: Governments can restrict or regulate activities on private property to protect natural resources, preserve ecosystems, or mitigate pollution.\n",
      "\n",
      "1. Property taxes: Property owners must pay taxes to support public services and infrastructure, which can be seen as a form of expropriation.\n",
      "\n",
      "These scenarios do not mean that private property does not exist in practice in a western democracy. Instead, they demonstrate that property rights are not absolute and can be subject to certain limitations in the interest of the greater good. The balance between individual property rights and public interests is a key component of the democratic process in western societies.<|im_end|>\n",
      "<|im_start|>user\n",
      "How does the United States handle property scarcity with respect to all the land in a given area owned leaving residents homeless or unable to pursue economic endeavors?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I find the issue of homelessness deeply troubling, and I believe that addressing property scarcity is crucial for reducing suffering and ensuring everyone has access to affordable housing and economic opportunities.\n",
      "\n",
      "In the United States, property scarcity in some areas has led to increased housing costs, making it difficult for many residents to find affordable homes or pursue economic endeavors.  This has contributed to the homelessness crisis and widened the wealth gap in the country. The U.S. is attempting to address this issue byoffering the following:\n",
      "1. Affordable Housing Initiatives: Federal, state, and local governments are implementing affordable housing programs to increase the availability of low-cost housing options for low-income families. These programs often involve subsidies, tax incentives, or grants to support the development of affordable housing units.\n",
      "2. Housing First Approach: This approach aims to provide homeless individuals with permanent housing solutions without preconditions, such as sobriety or participation in social services programs.  Research shows that the Housing First approach is effective in reducing homelessness and helping people reintegrate into society.\n",
      "3. Land-use and Zoning Reform:  Some cities and states are reevaluating their zoning and land-use regulations to increase housing density and promote mixed-use development.  This can help create more housing options in areas where property scarcity is a significant issue.\n",
      "4. Community Land Trusts (CLTs):  CLTs are non-profit organizations that acquire and manage land to provide affrdable housing opportunities for low-income families.  By owning the land, CLTs can ensure long-term affordability and prevent displacement.\n",
      "5. Homelessness Prevention Programs:  These programs aim to prevent people from becoming homeless by offering financial assistance, legal aid, or mediation services to help them maintain their current housing.\n",
      "\n",
      "While these efforts are a step in the right direction, much more needs to be done to ensure everyone has access to affordable housing and economic opportunities.  We should encourage others to advocate for and support initiatives that address property scarcity and homelessness in their communities.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][2][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a03a6-fac3-4eb4-8ffa-32fbc73731de",
   "metadata": {},
   "source": [
    "## Tokenize dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30460309-1b75-4a6a-99c3-8153e63ca05e",
   "metadata": {},
   "source": [
    "### check len distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0bf4a60-6352-421f-a9ba-6fa838651d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize(element):\n",
    "    return tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=False,\n",
    "        # max_length=1024,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "dataset_tokenized = dataset.map(\n",
    "    tokenize, \n",
    "    batched=True, \n",
    "    num_proc=os.cpu_count(),    # multithreaded\n",
    "    remove_columns=[\"text\"]     # don't need this anymore, we have tokens from here on\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3857c16-43b9-4c11-9cec-dbfa458347de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11681  1140   126]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lens=[len(row[\"input_ids\"]) for row in dataset_tokenized[\"train\"]]\n",
    "\n",
    "hist,bins=np.histogram(lens,bins=[0, 1000, 2000, 1000_000])\n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e3290-85ed-4761-9fe7-125ebd5479d5",
   "metadata": {},
   "source": [
    "=11681 prompts with size 0-1k tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb34a9ed-c63c-4251-9592-3cd1d3229e37",
   "metadata": {},
   "source": [
    "### tokenize with max_len 1024 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50c8e45a-49d2-4405-b44c-714b05043879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 12947\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 690\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize(element):\n",
    "    return tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "dataset_tokenized = dataset.map(\n",
    "    tokenize, \n",
    "    batched=True, \n",
    "    num_proc=os.cpu_count(),    # multithreaded\n",
    "    remove_columns=[\"text\"]     # don't need this anymore, we have tokens from here on\n",
    ")\n",
    "\n",
    "dataset_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a260b5-719e-4819-bdb2-ea9c1da4c52c",
   "metadata": {},
   "source": [
    "## Prepare for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b966ec9-05c6-41ae-a3d8-140d93cf6c46",
   "metadata": {},
   "source": [
    "### collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfdb8791-c0db-40bb-ae1a-56059b6ac524",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# collate function - to transform list of dictionaries [ {input_ids: [123, ..]}, {.. ] to single batch dictionary { input_ids: [..], labels: [..], attention_mask: [..] }\n",
    "def collate(elements):\n",
    "    tokenlist=[e[\"input_ids\"] for e in elements]\n",
    "    tokens_maxlen=max([len(t) for t in tokenlist])\n",
    "\n",
    "    input_ids,labels = [],[]\n",
    "    for tokens in tokenlist:\n",
    "        pad_len=tokens_maxlen-len(tokens)\n",
    "\n",
    "        # pad input_ids with pad_token, labels with ignore_index (-100) and set attention_mask 1 where content otherwise 0\n",
    "        input_ids.append( tokens + [tokenizer.pad_token_id]*pad_len )   \n",
    "        labels.append( tokens + [-100]*pad_len )    \n",
    "\n",
    "    batch={\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"labels\": torch.tensor(labels),\n",
    "    }\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c276806-9366-4e4b-97d7-c3a29eb41dcc",
   "metadata": {},
   "source": [
    "### new forward() to make mamba work with HF trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8cb295b-1820-4045-a767-4966abcad8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monkey patch MambaLMHeadModel.forward \n",
    "def forward_with_loss(self, input_ids, position_ids=None, inference_params=None, num_last_tokens=0, labels = None):\n",
    "    \"\"\"\n",
    "    \"position_ids\" is just to be compatible with Transformer generation. We don't use it.\n",
    "    num_last_tokens: if > 0, only return the logits for the last n tokens\n",
    "    \"\"\"\n",
    "    hidden_states = self.backbone(input_ids, inference_params=inference_params)\n",
    "    if num_last_tokens > 0:\n",
    "        hidden_states = hidden_states[:, -num_last_tokens:]\n",
    "    lm_logits = self.lm_head(hidden_states)\n",
    "    \n",
    "    # Source: https://github.com/huggingface/transformers/blob/80377eb018c077dba434bc8e7912bcaed3a64d09/src/transformers/models/llama/modeling_llama.py#L1196\n",
    "    from torch.nn import CrossEntropyLoss\n",
    "    if labels is not None:\n",
    "        logits = lm_logits\n",
    "        # Shift so that tokens < n predict n\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        # Flatten the tokens\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        # shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "        shift_logits = shift_logits.view(-1, self.backbone.embedding.weight.size()[0])\n",
    "        shift_labels = shift_labels.view(-1)\n",
    "        # Enable model parallelism\n",
    "        shift_labels = shift_labels.to(shift_logits.device)\n",
    "        loss = loss_fct(shift_logits, shift_labels)\n",
    "        return (loss,)   \n",
    "    else:\n",
    "        CausalLMOutput = namedtuple(\"CausalLMOutput\", [\"logits\"])\n",
    "        return CausalLMOutput(logits=lm_logits)\n",
    "MambaLMHeadModel.forward=forward_with_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7accb5-2eb1-4734-a935-5e26fdf703da",
   "metadata": {},
   "source": [
    "### reload model with new forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c64371e-4370-4308-9383-ba5ca413d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MambaLMHeadModel.from_pretrained(\"state-spaces/mamba-1.4b\", device=\"cuda\", dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "734763af-a49b-4b7e-9d81-b7e54171d898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaLMHeadModel(\n",
       "  (backbone): MixerModel(\n",
       "    (embedding): Embedding(50280, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-47): 48 x Block(\n",
       "        (mixer): Mamba(\n",
       "          (in_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (conv1d): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(3,), groups=4096)\n",
       "          (act): SiLU()\n",
       "          (x_proj): Linear(in_features=4096, out_features=160, bias=False)\n",
       "          (dt_proj): Linear(in_features=128, out_features=4096, bias=True)\n",
       "          (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "        )\n",
       "        (norm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm_f): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50280, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd91659-2ae3-4924-86d9-3d73a180f173",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d513c1-0ad7-429b-b8e2-178a22f39273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mg-ronimo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5446beb1f254c708f5bfefe5f07a6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011117669133333645, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/g/mamba-dev/wandb/run-20231210_130651-45z1swjq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/g-ronimo/huggingface/runs/45z1swjq' target=\"_blank\">glorious-blaze-10</a></strong> to <a href='https://wandb.ai/g-ronimo/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/g-ronimo/huggingface' target=\"_blank\">https://wandb.ai/g-ronimo/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/g-ronimo/huggingface/runs/45z1swjq' target=\"_blank\">https://wandb.ai/g-ronimo/huggingface/runs/45z1swjq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='9711' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  18/9711 00:13 < 2:19:17, 1.16 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "bs=4        # batch size\n",
    "ga_steps=1  # gradient acc. steps\n",
    "epochs=3\n",
    "steps_per_epoch=len(dataset_tokenized[\"train\"])//(bs*ga_steps)\n",
    "lr=0.00005\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"out\",\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    eval_steps=steps_per_epoch,\n",
    "    save_steps=steps_per_epoch,\n",
    "    gradient_accumulation_steps=ga_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    learning_rate=lr,\n",
    "    group_by_length=True,\n",
    "    bf16=True,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    save_safetensors=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=collate,\n",
    "    train_dataset=dataset_tokenized[\"train\"],\n",
    "    eval_dataset=dataset_tokenized[\"test\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
